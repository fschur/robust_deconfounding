import numpy as np
import random, pickle, json, os
from pygam import LinearGAM, s

from utils_experiments import get_results, get_data
from synthetic_data import functions_nonlinear
from robust_deconfounding.utils import get_funcbasis

"""
The experiments can take up to an hour, therefore the results are saved in the folder "results".
The configuration for the different experiments discussed in the paper are saved in the config.json file and can be chosen throught the "exp" variable.
New experiments can be run by simply extending the config.json file.
Plots can afterwards be generated by running the plot_nonlinear.py file with the same "exp" variable.

Short explanation of the variables and what they do:
"unifrom" : X_t is i.i.d. uniformly distributed on [0,1]
"reflected_ou" : X_t is a reflected Ornstein-Uhlenbeck process
"""

exp="uniform"     # "uniform" | "reflected_ou"

# ----------------------------------
# Set up and pepare Monte Carlo simluation
# ----------------------------------

SEED = 1
np.random.seed(SEED)
random.seed(SEED)

path_results=os.path.join(os.path.dirname(__file__), "results/")        #Path to the results
path= os.path.dirname(__file__)   #Path for the json file where experiment configurations are defined.

with open(path+'/config.json', 'r') as file:
    config = json.load(file)

config=config["experiment_"+str(exp)]
data_args, method_args, m, noise_vars, L_frac, num_data= config["data_args"], config["method_args"], config["m"], np.array(config["noise_vars"]), np.array(config["L_frac"]), np.array(config["num_data"])   
data_args["beta"]=np.array(data_args["beta"])        

n_x=200                     #Resolution of x-axis
int_test=[0.1, 0.9]         #Interval [a,b] on which the L^1-error is computed
len_test=int_test[1]-int_test[0]
test_points=np.array([int_test[0]+ i/(n_x)*len_test for i in range(n_x)])
y_true=functions_nonlinear(np.ndarray((n_x,1), buffer=test_points), data_args["beta"][0])   #Compute true underlying f(x)


# ----------------------------------
# run experiments
# ----------------------------------

for i in range(len(noise_vars)):
    print("Noise Variance: ", noise_vars[i])
    res = {"DecoR": [], "ols": []}       

    for n in num_data:
        print("number of data points: ", n)
        res["DecoR"].append([])
        res["ols"].append([])
        L=max((np.ceil(n**0.5)/L_frac[i]).astype(int),2)
        basis=get_funcbasis(x=test_points, L=L, type=method_args["basis_type"])
        print("number of coefficients: ", L)

        for _ in range(m):
            data_values = get_data(n, **data_args, noise_var=noise_vars[i])
            estimates_decor = get_results(**data_values, **method_args, nonlinear=True, L=L)
            y_est=basis @ estimates_decor
            y_est=np.ndarray((n_x, 1), buffer=y_est)

            gam = LinearGAM(s(0)).gridsearch(np.reshape(data_values["x"], (-1,1)), data_values["y"])
            y_bench=gam.predict(test_points)
           
            res["DecoR"][-1].append(1/n_x*len_test*np.linalg.norm(y_true-y_est, ord=1))
            res["ols"][-1].append(1/n_x*len_test*np.linalg.norm(y_true-y_bench, ord=1))

    #Save the results using a pickle file
    res["DecoR"], res["ols"] = np.array(res["DecoR"]), np.array(res["ols"])
    with open(path_results+"experiment_" + exp +'_noise_='+str(noise_vars[i])+'.pkl', 'wb') as fp:
        pickle.dump(res, fp)
        print('Results saved successfully to file.')
